{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ffea3e-47b4-4548-bf19-24d57f98c5da",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d725c75-ef09-4a1f-8fcf-2e5b98a8ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Pandas: 2.2.3\n",
      "Scikit-learn: 1.7.0\n",
      "spacy: 3.8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"Scikit-learn:\", sklearn.__version__)\n",
    "print(\"spacy:\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "371f6f86-3dd3-4241-89c4-2c894b4373a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to combine words to make sentence\n",
    "def words_to_sentence(file_path_word, file_path_label):\n",
    "    current_sentence = []\n",
    "    all_sentence = []\n",
    "    sentence_list = []\n",
    "    label_list = []\n",
    "    current_sentence = []\n",
    "    current_label = []\n",
    "    with open(file_path_word, mode='r', encoding='utf-8') as word_file, open(file_path_label, mode='r', encoding='utf-8') as label_file:\n",
    "        for word_line, label_line in zip(word_file, label_file):\n",
    "            word = word_line.strip()\n",
    "            label = label_line.strip()\n",
    "            \n",
    "            # When empty line entered i.e end of sentence.\n",
    "            if (word == '' and label == ''):\n",
    "                if(current_sentence and current_label):\n",
    "                    sentence_list.append(current_sentence)\n",
    "                    label_list.append(current_label)\n",
    "                    current_sentence = []\n",
    "                    current_label = []\n",
    "            else:\n",
    "                current_sentence.append(word)\n",
    "                current_label.append(label)\n",
    "\n",
    "        # For the last sentence where no empty line after is present\n",
    "        if(current_sentence and current_label):\n",
    "            sentence_list.append(current_sentence)\n",
    "            label_list.append(current_label)\n",
    "\n",
    "    return sentence_list, label_list\n",
    "\n",
    "def isValid(token):\n",
    "    if(len(token.text) > 2 and token.pos_ in ['NOUN','PROPN'] and not token.is_stop and not token.is_punct):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def create_tagged_data(sentences, labels):\n",
    "    tagged_sentences = []\n",
    "    for tokens, tags in zip(sentences, labels):\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        tagged = list(zip(tokens, pos_tags, tags))\n",
    "        tagged_sentences.append(tagged)\n",
    "    return tagged_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15236422-c292-4094-b055-389353833ca1",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "252e550a-c0a8-4a2c-b4fb-f2f7c715b1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All live births > or = 23 weeks at the University of Vermont in 1995 ( n = 2395 ) were retrospectively analyzed for delivery route , indication for cesarean , gestational age , parity , and practice group ( to reflect risk status ) - O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Furthermore , when all deliveries were analyzed , regardless of risk status but limited to gestational age > or = 36 weeks , the rates did not change ( 12.6 % , 280 of 2214 ; primary 9.2 % , 183 of 1994 ) - O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "The total cesarean rate was 14.4 % ( 344 of 2395 ) , and the primary rate was 11.4 % ( 244 of 2144 ) - O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "As the ambient temperature increases , there is an increase in insensible fluid loss and the potential for dehydration - O O O O O O O O O O O O O O O O O O O\n",
      "Abnormal presentation was the most common indication ( 25.6 % , 88 of 344 ) - O O O O O O O O O O O O O O O\n",
      "The daily high temperature ranged from 71 to 104 degrees F and AFI values ranged from 1.7 to 24.7 cm during the study period - O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "The `` corrected '' cesarean rate ( maternal-fetal medicine and transported patients excluded ) was 12.4 % ( 273 of 2194 ) , and the `` corrected '' primary rate was 9.6 % ( 190 of 1975 ) - O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "There was a significant correlation between the 2- , 3- , and 4-day mean temperature and AFI , with the 4-day mean being the most significant ( r = 0.31 , p & # 60 ; 0.001 ) - O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "Arrest of dilation was the most common indication in both `` corrected '' subgroups ( 23.4 and 24.6 % , respectively ) - O O O O O O O O O O O O O O O O O O O O O O\n",
      "Fluctuations in ambient temperature are inversely correlated to changes in AFI - O O O O O O O O O O O\n",
      "no. of train sentence: 2599\n",
      "no. of train label: 2599\n",
      "no. of test sentence: 1056\n",
      "no. of test label: 1056\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels = words_to_sentence('train_sent', 'train_label')\n",
    "test_sentences, test_labels = words_to_sentence('test_sent', 'test_label')\n",
    "\n",
    "# Print first 5 sentences and labels\n",
    "for i in range(5):\n",
    "    print(f\"{' '.join(train_sentences[i])} - {' '.join(train_labels[i])}\")\n",
    "    print(f\"{' '.join(test_sentences[i])} - {' '.join(test_labels[i])}\")\n",
    "    \n",
    "# Count no. of train and test data\n",
    "print(f\"no. of train sentence: {len(train_sentences)}\")\n",
    "print(f\"no. of train label: {len(train_labels)}\")\n",
    "print(f\"no. of test sentence: {len(test_sentences)}\")\n",
    "print(f\"no. of test label: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79213e-34e2-4772-9951-71adbbade57c",
   "metadata": {},
   "source": [
    "# Concept identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60362124-cac8-4780-b8c7-5cc14c5ffba6",
   "metadata": {},
   "source": [
    "## Print frequency of NOUN and PROPN and 25 most used NOUN and PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c25b73a-46b2-4412-9c15-de52b0e74ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 25 most common used nouns\n",
      "patients -- 507\n",
      "treatment -- 304\n",
      "cancer -- 211\n",
      "therapy -- 177\n",
      "study -- 174\n",
      "disease -- 149\n",
      "cell -- 142\n",
      "lung -- 118\n",
      "results -- 116\n",
      "group -- 111\n",
      "effects -- 99\n",
      "gene -- 91\n",
      "chemotherapy -- 91\n",
      "use -- 87\n",
      "effect -- 82\n",
      "women -- 81\n",
      "analysis -- 76\n",
      "risk -- 74\n",
      "surgery -- 73\n",
      "cases -- 72\n",
      "rate -- 68\n",
      "survival -- 67\n",
      "response -- 66\n",
      "children -- 66\n",
      "dose -- 65\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_sentences = train_sentences + test_sentences\n",
    "noun_counter = Counter()\n",
    "\n",
    "for sentence in all_sentences:\n",
    "    tokenized_sent = nlp(\" \".join(sentence))\n",
    "    for token in tokenized_sent:\n",
    "        if isValid(token):\n",
    "            noun_counter[token.text.lower()] += 1\n",
    "            \n",
    "# Print nouns and their frequency\n",
    "# for word, count in noun_counter.items():\n",
    "#     print(f\"{word}-- {count}\")\n",
    "\n",
    "# Print 25 most common nouns\n",
    "print(f\"\\n 25 most common used nouns\")\n",
    "for word, count in noun_counter.most_common(25):\n",
    "    print(f\"{word} -- {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93189ab9-bb01-4e29-a7c8-0388f8ee1b1e",
   "metadata": {},
   "source": [
    "## Defining the features for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4aefa9d-c9c1-4dbc-834c-60d149b45660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesForOneWord(sentence, index):\n",
    "  word = sentence[index]\n",
    "\n",
    "  features = [\n",
    "    'word.lower=' + word.lower_, # serves as word id\n",
    "    'word.pos=' + word.pos_\n",
    "  ]\n",
    "\n",
    "  if(index > 0):\n",
    "    prev_word = sentence[index-1]\n",
    "    features.extend([\n",
    "    'prev_word.lower=' + prev_word.lower_,\n",
    "    'prev_word.pos=' + prev_word.pos_\n",
    "  ])\n",
    "  else:\n",
    "    features.append('BEG') # feature to track begin of sentence \n",
    "\n",
    "  if(index == len(sentence)-1):\n",
    "    features.append('END') # feature to track end of sentence\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1488a-b5b8-47a6-b3a5-7e791699368b",
   "metadata": {},
   "source": [
    "## Get features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ada71773-8ea6-437d-8d38-604610a520c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get features for a sentence \n",
    "# using the 'getFeaturesForOneWord' function.\n",
    "def getFeaturesForOneSentence(sentence):\n",
    "  sentence_list = nlp(sentence)\n",
    "  return [getFeaturesForOneWord(sentence_list, pos) for pos in range(len(sentence_list))]\n",
    "\n",
    "# Define a function to get the labels for a sentence.\n",
    "def getLabelsInListForOneSentence(labels):\n",
    "  return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "196e41c2-1276-4ae6-ab58-df1a5837f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [getFeaturesForOneSentence(\"\".join(sentence)) for sentence in train_sentences]\n",
    "Y_train = [getLabelsInListForOneSentence(labels) for labels in train_labels]\n",
    "\n",
    "X_test = [getFeaturesForOneSentence(\"\".join(sentence)) for sentence in test_sentences]\n",
    "Y_test = [getLabelsInListForOneSentence(labels) for labels in test_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fb419444-7ed1-4d7d-9417-03b3e1f518c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(Y_test)) \n",
    "filtered = [(x, y) for x, y in zip(X_train, Y_train) if len(x) == len(y)]\n",
    "filtered_test = [(x, y) for x, y in zip(X_test, Y_test) if len(x) == len(y)]\n",
    "X_train, Y_train = map(list, zip(*filtered))\n",
    "X_test, Y_test = map(list, zip(*filtered_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5cff4b-9083-4298-891a-8ce2c2416242",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fee6e95e-4965-469a-acef-1c5c819c1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(max_iterations=100)\n",
    "crf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(Y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8855ca6-37ba-414e-b001-940cd8d39b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the label distribution\n",
    "flat_labels = [label for sent in y_train_filtered for label in sent]\n",
    "label_counts = Counter(flat_labels)\n",
    "\n",
    "print(\"Count of labels:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cd5c3978-1da9-4f54-9a72-c106e1b1ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Sequelae', 'include', 'severe', 'developmental', 'delay', 'and', 'asymmetric', 'double', 'hemiplegia']\n",
      "Sentence length: 9\n",
      "Orig Labels: ['O', 'O', 'O', 'O', 'O']\n",
      "Pred Labels: ['O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "id = 20\n",
    "print(\"Sentence:\", test_sentences[id])\n",
    "print(\"Sentence length:\", len(test_sentences[id]))\n",
    "print(\"Orig Labels:\", Y_test[id])\n",
    "print(\"Pred Labels:\", y_pred[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7488fb77-c845-4cde-a6af-b64a7f06dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionary for mapping diseases to their treatments\n",
    "disease_to_treatments = defaultdict(list)\n",
    "\n",
    "for tokens, preds in zip(test_sentences, y_pred):\n",
    "    # Find all diseases in this sentence\n",
    "    for idx, (token, label) in enumerate(zip(tokens, preds)):\n",
    "        if label == 'D':\n",
    "            disease = token\n",
    "            # Find all treatments in the same sentence\n",
    "            treatments = [tokens[j] for j, l in enumerate(preds) if l == 'T']\n",
    "            # You may want to avoid duplicates for each disease\n",
    "            for treatment in treatments:\n",
    "                if treatment not in disease_to_treatments[disease]:\n",
    "                    disease_to_treatments[disease].append(treatment)\n",
    "\n",
    "# Convert to normal dict for display\n",
    "disease_to_treatments = dict(disease_to_treatments)\n",
    "print(disease_to_treatments)\n",
    "for d, tlist in disease_to_treatments.items():\n",
    "    print(f\"{d}: {', '.join(tlist)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4768f74-ee01-4d76-a7d9-dc4367c1fe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
